[
  {
    "objectID": "rscripts.html",
    "href": "rscripts.html",
    "title": "R (and other) scripts",
    "section": "",
    "text": "Download R code by chapter\n\nChapter 1: Introduction\nChapter 2: Binary recursive partitioning with CART\nChapter 3: Conditional inference trees\nChapter 4: The hitchhiker’s GUIDE to modern decision trees\nChapter 5: Ensemble algorithms\nChapter 6: Peeking inside the “black box”: post-hoc interpretability\nChapter 7: Random forests\nChapter 8: Gradient boosting machines\n\nNGBoost example in Python"
  },
  {
    "objectID": "posts/test-2.html",
    "href": "posts/test-2.html",
    "title": "Tree-Based Methods for Statistical Learning",
    "section": "",
    "text": "Just a test post…"
  },
  {
    "objectID": "posts/test-1.html",
    "href": "posts/test-1.html",
    "title": "Class Imbalance & Altered Priors",
    "section": "",
    "text": "ABC.\nABC.\n1 + 1\n\nlibrary(pdp)\n\nsummary(lm(cmedv ~ ., data = boston))\n\n\nCall:\nlm(formula = cmedv ~ ., data = boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5831  -2.7643  -0.5994   1.7482  26.0822 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.350e+02  3.032e+02  -1.435 0.152029    \nlon         -3.935e+00  3.372e+00  -1.167 0.243770    \nlat          4.495e+00  3.669e+00   1.225 0.221055    \ncrim        -1.045e-01  3.261e-02  -3.206 0.001436 ** \nzn           4.657e-02  1.374e-02   3.390 0.000755 ***\nindus        1.524e-02  6.175e-02   0.247 0.805106    \nchas1        2.578e+00  8.650e-01   2.980 0.003024 ** \nnox         -1.582e+01  4.005e+00  -3.951 8.93e-05 ***\nrm           3.754e+00  4.166e-01   9.011  < 2e-16 ***\nage          2.468e-03  1.335e-02   0.185 0.853440    \ndis         -1.400e+00  2.088e-01  -6.704 5.61e-11 ***\nrad          3.067e-01  6.658e-02   4.607 5.23e-06 ***\ntax         -1.289e-02  3.727e-03  -3.458 0.000592 ***\nptratio     -8.771e-01  1.363e-01  -6.436 2.92e-10 ***\nb            9.176e-03  2.663e-03   3.446 0.000618 ***\nlstat       -5.374e-01  5.042e-02 -10.660  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.7 on 490 degrees of freedom\nMultiple R-squared:  0.7458,    Adjusted R-squared:  0.738 \nF-statistic: 95.82 on 15 and 490 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "supplementary.html",
    "href": "supplementary.html",
    "title": "Online supplementary material",
    "section": "",
    "text": "TBD…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this book",
    "section": "",
    "text": "WARNING: This site is still very much a work in progress!\nWelcome to Tree-Based Methods for Statistical Learning in R. Tree-based methods, as viewed in this book, refer to a broad family of algorithms that rely on decision trees, of which this book attempts to provide a thorough treatment. This is not a general statistical or machine learning book, nor is it an R book. Consequently, some familiarity with both would be useful, but I’ve tried to keep the core material as accessible and practical as possible to a broad audience (even if you’re not an R programmer or master of statistical and machine learning). That being said, I’m a firm believer in learning by doing, and in understanding concept through code examples. To that end, almost every major section in this book is followed-up by general programming examples to help further drive the material home. Therefore, this book necessarily involves a lot of code snippets.\nThis website is where I plan to include chapter exercises, code to reproduce most of the examples and figures in the book, errata, and various supplementary material.\nContributions from the community are more than welcome! If you notice something is missing from the website (e.g., the code to reproduce one of the figures or examples) or notice an issue in the book (e.g., typos or problems with the material), please don’t hesitate to reach out. A good place to report such problems is the companion website’s GitHub issues tab.\nEven if it’s a section of the material you found confusing or hard to understand, I want to hear about it!"
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "About this book",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is primarily aimed at researchers and practitioners who want to go beyond a fundamental understanding of tree-based methods, such as decision trees and tree-based ensembles. It could also serve as a useful supplementary text for a graduate level course on statistical and machine learning. Some parts of the book necessarily involve more math and notation than others, but where possible, I try to use code to make the concepts more comprehensible. For example, Chapter 3 on conditional inference trees involves a bit of linear algebra and intimidating matrix notation, but the math-oriented sections can often be skipped without sacrificing too much in the way of understanding the core concepts; the adjacent code examples should also help drive the main concepts home by connecting the math to simple coding logic.\nNonetheless, this book does assume some familiarity with the basics of statistical and machine learning, as well as the R programming language. Useful references and resources are provided in the introductory material in Chapter 1. While I try to provide sufficient detail and background where possible, some topics could only be given cursory treatment, though, whenever possible, I try to point the more ambitious reader in the right direction in terms of references."
  },
  {
    "objectID": "index.html#the-treemisc-package",
    "href": "index.html#the-treemisc-package",
    "title": "About this book",
    "section": "The treemisc package",
    "text": "The treemisc package\nAlong with the companion website, there’s also a companion R package, called `treemisc`, that houses a number of the data sets and functions used throughout this book. Installation instructions and documentation can be found in the package’s GitHub repository\nTo install directly from GitHub, use:\n\n# install.packages(\"remotes\")  # requires remotes package\nremotes::install_github('bgreenwell/treemisc')"
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "About this book",
    "section": "About the author",
    "text": "About the author\nI’m a data scientist at 84.51° where I work to enable, empower, and enculturate statistical and machine learning best practices where it’s applicable to help others solve real business problems. I received a B.S. in Statistics and an M.S. in Applied Statistics from Wright State University, and a Ph.D. in Applied Mathematics from the Air Force Institute of Technology. I was part of the Adjunct Graduate Faculty at Wright State University, and currently an Adjunct Instructor at the University of Cincinnati. I’m also the lead developer and maintainer of several R packages available on CRAN (and off CRAN), and co-author of Hands-On Machine Learning with R."
  },
  {
    "objectID": "index.html#download-a-sample",
    "href": "index.html#download-a-sample",
    "title": "About this book",
    "section": "Download a sample",
    "text": "Download a sample\n\nBuilding a decision tree: Learn the details of building a CART-like decision tree beyond selecting the splitting variables. For example, learn how cost-complexity is computed (and how it slightly differs in rpart), or learn how pruning and the 1-SE (one standard error) rule are used with cross-validation to select a reasonably sized tree.\n“Deforesting” a random forest: Learn how to use the LASSO to post-process a random forest by effectively zeroing out some of the trees and reweighting the rest. The zeroed out trees can be removed from the forest object, resulting in a smaller, fast scoring model. The book also shows hot to accomplish this with gradient boosted tree ensembles in a later chapter (there’s a couple of subtle differences involved).\nGradient tree boosting from scratch: Gain a deeper understanding (and appreciation) for how gradient tree boosting works by writing your own gradient boosting functions to “boost” a single rpart tree using both least squares (LS) and least absolute deviation (LAD) loss; the latter requires performing the line search step of gradient descent by updating the terminal node estimates of each fitted tree (the code shows two ways fo doing this)."
  },
  {
    "objectID": "index.html#reviews",
    "href": "index.html#reviews",
    "title": "About this book",
    "section": "Review(s)",
    "text": "Review(s)\nTree-based algorithms have been a workhorse for data science teams for decades, but the data science field has lacked an all-encompassing review of trees — and their modern variants like XGBoost — until now. Greenwell has written the ultimate guide for tree-based methods: how they work, their pitfalls, and alternative solutions. He puts it all together in a readable and immediately usable book. You’re guaranteed to learn new tips and tricks to help your data science team.\n-Alex Gutman, Director of Data Science, Author: Becoming a Data Head: How to Think, Speak and Understand Data Science, Statistics and Machine Learning"
  },
  {
    "objectID": "howtoorder.html",
    "href": "howtoorder.html",
    "title": "Ordering information",
    "section": "",
    "text": "You can order the book from one of the vendors listed below (click the icon to link to the respective vendor):"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Data sets used in this book",
    "section": "",
    "text": "Information pertaining to several of the data sets used throughout the book."
  },
  {
    "objectID": "datasets.html#swiss-banknote-data",
    "href": "datasets.html#swiss-banknote-data",
    "title": "Data sets used in this book",
    "section": "Swiss banknote data",
    "text": "Swiss banknote data\nThe Swiss banknote data contain measurements from 200 Swiss 1000-franc banknotes: 100 genuine (y = 0) and 100 counterfeit (y = 1). For R users, the data are conveniently available as the banknote data frame in package treemisc, and can be loaded using\n\nhead(bn <- treemisc::banknote)\n\n  length  left right bottom  top diagonal y\n1  214.8 131.0 131.1    9.0  9.7    141.0 0\n2  214.6 129.7 129.7    8.1  9.5    141.7 0\n3  214.8 129.7 129.7    8.7  9.6    142.2 0\n4  214.8 129.7 129.6    7.5 10.4    142.0 0\n5  215.0 129.6 129.7   10.4  7.7    141.8 0\n6  215.7 130.8 130.5    9.0 10.1    141.4 0\n\n\nDownload: banknote.csv\nReferences\nFlury, B. and Riedwyl, H. (1988). Multivariate Statistics: A practical approach. London: Chapman & Hall, Tables 1.1 and 1.2, pp. 5-8."
  },
  {
    "objectID": "datasets.html#new-york-air-quality-measurements",
    "href": "datasets.html#new-york-air-quality-measurements",
    "title": "Data sets used in this book",
    "section": "New York air quality measurements",
    "text": "New York air quality measurements\nThe New York air quality data contain daily air quality measurements in New York from May through September of 1973 (153 days). The data are conveniently available in R’s built-in datasets package; see ?datasets::airquality for details and the original source. The main variables include:\n\nOzone: the mean ozone (in parts per billion) from 1300 to 1500 hours at Roosevelt Island;\nSolar.R: the solar radiation (in Langleys) in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park;\nWind: the average wind speed (in miles per hour) at 0700 and 1000 hours at LaGuardia Airport;\nTemp: the maximum daily temperature (in degrees Fahrenheit) at La Guardia Airport.\n\nThe month (1–12) and day of the month (1–31) are also available in the columns Monthand Day, respectively. In these data, Ozone is treated as a response variable.\n\nhead(aq <- airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nDownload: airquality.csv"
  },
  {
    "objectID": "datasets.html#the-friedman-1-benchmark-data",
    "href": "datasets.html#the-friedman-1-benchmark-data",
    "title": "Data sets used in this book",
    "section": "The Friedman 1 benchmark data",
    "text": "The Friedman 1 benchmark data\nThe Friedman 1 benchmark problem uses simulated regression data with 10 input features according to:\n\\[\nY = 10 \\sin\\left(\\pi X_1 X_2\\right) + 20 \\left(X_3 - 0.5\\right) ^ 2 + 10 X_4 + 5 X_5 + \\epsilon,\n\\]\nwhere \\(\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma\\right)\\) and the input features are all independent uniform random variables on the interval \\(\\left[0, 1\\right]\\): \\(\\left\\{X_j\\right\\}_{j = 1}^10 \\stackrel{iid}{\\sim} \\mathcal{U}\\left(0, 1\\right)\\). Notice how \\(X_6\\)–\\(X_{10}\\) are unrelated to the response \\(Y\\).\nThese data can be generated in R using the mlbench.friedman1() function from package mlbench. Here, I’ll use the gen_friedman1() function from package treemisc which allows you to generate any number of features \\(\\ge 5\\); similar to the make\\_friedman1() function in scikit-learn’s sklearn.datasets module for Python. See ?treemisc::gen_friedman1 for details.\n\nset.seed(943)  # for reproducibility\ntreemisc::gen_friedman1(5, nx = 7, sigma = 0.1)\n\n         y        x1        x2        x3        x4        x5         x6\n1 18.47570 0.3459904 0.8530512 0.6551429 0.8385246 0.2930157 0.34084319\n2 13.72402 0.4419899 0.6912439 0.2136197 0.1077115 0.5432351 0.16164747\n3 10.84887 0.2225216 0.7893439 0.8068398 0.2515591 0.2566107 0.85946717\n4 18.93996 0.8594261 0.5196663 0.8911978 0.1285491 0.9356575 0.03476787\n5 14.46567 0.1808317 0.5901250 0.8934141 0.6110807 0.4151859 0.41041196\n          x7\n1 0.05733414\n2 0.50547584\n3 0.72484074\n4 0.71050552\n5 0.26356893\n\n\nSource code:\n\ntreemisc::gen_friedman1\n\nfunction(n = 100, nx = 10, sigma = 0.1) {\n  if (nx < 5) {\n    stop(\"`nsim` must be >= 5.\", call. = FALSE)\n  }\n  x <- matrix(stats::runif(n * nx), ncol = nx)\n  colnames(x) <- paste0(\"x\", seq_len(nx))\n  y = 10 * sin(pi * x[, 1L] * x[, 2L]) + 20 * (x[, 3L] - 0.5) ^ 2 +\n    10 * x[, 4L] + 5 * x[, 5L] + stats::rnorm(n, sd = sigma)\n  as.data.frame(cbind(y = y, x))\n}\n<bytecode: 0x7fa90ff562e0>\n<environment: namespace:treemisc>"
  },
  {
    "objectID": "errata.html",
    "href": "errata.html",
    "title": "Errata",
    "section": "",
    "text": "Errata for the 1st edition, first printing (June, 2022)\n\nPage 9. At the top pf the page, “…polynomial modelpolynomial” should be “…polynomial model.” (Thanks to @RaymondBalise)\nPage 9. Just below the figure caption, “…MSE” should be “…MSE (mean squared error).” (Thanks to @RaymondBalise)\nPage 22. In the last paragraph, “(red curve)” should be “(black curve).” (Thanks to @RaymondBalise)\nPage 48. Just before the function at the bottom of the page, “…” should be “…see ?`[`.” (Thanks to @RaymondBalise)\nPage 72. In the second to last paragraph, “…take \\(\\mathcal{T}_0\\) to be the left tree in Figure 2.12” should be “…take T_0 to be the left tree in Figure 2.13.” (Thanks to @RaymondBalise);\nPage 75. The phrase “…nodes \\(A_5\\)–\\(A_7\\)” is confusing (since the tree nodes are not labeled left to right and top to bottom) and should probably be changed to “…nodes \\(A_9\\), \\(A_5\\), and \\(A_3\\)”.\nPage 97. “…Cleveland dot plot displayed in Figure ??” should be “…Cleveland dot plot displayed in Figure 2.24. (Thanks to @RaymondBalise)\nPage 128. Missing closing parentheses at the end of the first paragraph in Section 3.4.4.\nPage 197. Towards the bottom of the page, “…in an ordinary bagged tree ensemble).” should be “…in an ordinary bagged tree ensemble” (random closing parentheses).\nPage 276. The last line of Section 7.8 uses notation that’s inconsistent with the notation in Section 6.3. In particular, “…to the difference in \\(f\\left(x^\\star\\right) - E\\left[\\hat{f}\\left(x\\right)\\right] = 0.51\\)” should probably be changed to “…to the difference \\(\\hat{f}\\left(\\boldsymbol{x}^\\star\\right) - \\bar{f} = 0.51\\).”\nPage 301. In footnote “w”, sparkR should be SparkR (i.e., it’s spelled in upper camel case)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Invalid Date\n\n\nBrandon M. Greenwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]